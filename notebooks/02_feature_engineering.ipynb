{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d63e606",
   "metadata": {},
   "source": [
    "# Feature Engineering for Volatility Prediction\n",
    "\n",
    "This notebook creates comprehensive features for volatility prediction including:\n",
    "\n",
    "## Key Features:\n",
    "- Technical indicators (RSI, MACD, Bollinger Bands, etc.)\n",
    "- Volatility estimators (Parkinson, Garman-Klass, etc.)\n",
    "- Regime detection features\n",
    "- Calendar and seasonal effects\n",
    "- Cross-asset spillover effects\n",
    "- Market microstructure indicators\n",
    "- Indian market specific features (VIX, FII/DII flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776d00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d105d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "from src.config.settings import get_config\n",
    "from src.features.basic_features import BasicFeatureEngineer\n",
    "from src.features.advanced_features import AdvancedFeatureEngineer\n",
    "from src.features.volatility_estimators import VolatilityEstimators\n",
    "from src.utils.logging import get_logger\n",
    "\n",
    "# Initialize configuration and logger\n",
    "config = get_config()\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "print(\"Project modules imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b36315",
   "metadata": {},
   "source": [
    "## 1. Load Previously Downloaded Data\n",
    "\n",
    "Load the data from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d32fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data directories\n",
    "raw_data_dir = os.path.join(project_root, 'data', 'raw')\n",
    "processed_data_dir = os.path.join(project_root, 'data', 'processed')\n",
    "\n",
    "print(f\"Looking for data in:\")\n",
    "print(f\"  Raw: {raw_data_dir}\")\n",
    "print(f\"  Processed: {processed_data_dir}\")\n",
    "\n",
    "# Check if directories exist\n",
    "if not os.path.exists(raw_data_dir):\n",
    "    print(f\"⚠ Raw data directory not found. Please run 01_data_download.ipynb first.\")\n",
    "else:\n",
    "    print(f\"✓ Raw data directory found\")\n",
    "    \n",
    "if not os.path.exists(processed_data_dir):\n",
    "    print(f\"⚠ Processed data directory not found. Please run 01_data_download.ipynb first.\")\n",
    "else:\n",
    "    print(f\"✓ Processed data directory found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e397762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stock data\n",
    "stock_data = {}\n",
    "data_files = [f for f in os.listdir(raw_data_dir) if f.endswith('_stock_data.csv')]\n",
    "\n",
    "print(f\"Found {len(data_files)} stock data files:\")\n",
    "\n",
    "for file in data_files:\n",
    "    symbol = file.replace('_stock_data.csv', '') + '.NS'\n",
    "    filepath = os.path.join(raw_data_dir, file)\n",
    "    \n",
    "    try:\n",
    "        data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
    "        stock_data[symbol] = data\n",
    "        print(f\"  ✓ {symbol}: {len(data)} records\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Failed to load {symbol}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded data for {len(stock_data)} symbols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1a51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VIX data\n",
    "vix_file = os.path.join(raw_data_dir, 'india_vix_data.csv')\n",
    "vix_data = None\n",
    "\n",
    "if os.path.exists(vix_file):\n",
    "    try:\n",
    "        vix_data = pd.read_csv(vix_file, index_col=0, parse_dates=True)\n",
    "        print(f\"✓ Loaded VIX data: {len(vix_data)} records\")\n",
    "        print(f\"VIX columns: {list(vix_data.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load VIX data: {e}\")\n",
    "else:\n",
    "    print(\"⚠ VIX data file not found\")\n",
    "\n",
    "# Load volatility estimates\n",
    "vol_file = os.path.join(processed_data_dir, 'nifty50_volatility_estimates.csv')\n",
    "volatility_estimates = None\n",
    "\n",
    "if os.path.exists(vol_file):\n",
    "    try:\n",
    "        volatility_estimates = pd.read_csv(vol_file, index_col=0, parse_dates=True)\n",
    "        print(f\"✓ Loaded volatility estimates: {len(volatility_estimates)} records\")\n",
    "        print(f\"Volatility columns: {list(volatility_estimates.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load volatility estimates: {e}\")\n",
    "else:\n",
    "    print(\"⚠ Volatility estimates file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde6f5d8",
   "metadata": {},
   "source": [
    "## 2. Basic Feature Engineering\n",
    "\n",
    "Create basic features from price and volume data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd513e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize basic feature engineer\n",
    "basic_engineer = BasicFeatureEngineer()\n",
    "\n",
    "# Focus on NIFTY50 for detailed feature engineering\n",
    "target_symbol = 'NIFTY50.NS'\n",
    "if target_symbol not in stock_data:\n",
    "    # Fallback to first available symbol\n",
    "    target_symbol = list(stock_data.keys())[0]\n",
    "    print(f\"NIFTY50 not available, using {target_symbol} instead\")\n",
    "\n",
    "target_data = stock_data[target_symbol]\n",
    "print(f\"Creating features for {target_symbol}\")\n",
    "print(f\"Data shape: {target_data.shape}\")\n",
    "print(f\"Date range: {target_data.index[0]} to {target_data.index[-1]}\")\n",
    "print(f\"Columns: {list(target_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create basic features\n",
    "print(\"Creating basic features...\")\n",
    "\n",
    "basic_features = basic_engineer.create_features(target_data)\n",
    "\n",
    "print(f\"✓ Basic features created\")\n",
    "print(f\"Features shape: {basic_features.shape}\")\n",
    "print(f\"Feature columns: {list(basic_features.columns)}\")\n",
    "\n",
    "# Display sample features\n",
    "display(basic_features.head())\n",
    "print(\"\\nFeature summary:\")\n",
    "display(basic_features.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9fd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize basic features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Returns\n",
    "if 'returns' in basic_features.columns:\n",
    "    axes[0, 0].plot(basic_features.index, basic_features['returns'], alpha=0.7)\n",
    "    axes[0, 0].set_title('Daily Returns')\n",
    "    axes[0, 0].set_ylabel('Returns')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Log returns\n",
    "if 'log_returns' in basic_features.columns:\n",
    "    axes[0, 1].plot(basic_features.index, basic_features['log_returns'], alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_title('Log Returns')\n",
    "    axes[0, 1].set_ylabel('Log Returns')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Price momentum\n",
    "momentum_cols = [col for col in basic_features.columns if 'momentum' in col]\n",
    "if momentum_cols:\n",
    "    for col in momentum_cols[:3]:  # Plot first 3 momentum features\n",
    "        axes[0, 2].plot(basic_features.index, basic_features[col], label=col, alpha=0.8)\n",
    "    axes[0, 2].set_title('Price Momentum')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Volume features\n",
    "volume_cols = [col for col in basic_features.columns if 'volume' in col.lower()]\n",
    "if volume_cols:\n",
    "    for col in volume_cols[:3]:  # Plot first 3 volume features\n",
    "        axes[1, 0].plot(basic_features.index, basic_features[col], label=col, alpha=0.8)\n",
    "    axes[1, 0].set_title('Volume Features')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: High-Low features\n",
    "hl_cols = [col for col in basic_features.columns if any(x in col.lower() for x in ['high', 'low', 'range'])]\n",
    "if hl_cols:\n",
    "    for col in hl_cols[:3]:  # Plot first 3 high-low features\n",
    "        axes[1, 1].plot(basic_features.index, basic_features[col], label=col, alpha=0.8)\n",
    "    axes[1, 1].set_title('High-Low Features')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Feature correlation heatmap (sample)\n",
    "sample_features = basic_features.select_dtypes(include=[np.number]).iloc[:, :10]  # First 10 numeric features\n",
    "if not sample_features.empty:\n",
    "    corr_matrix = sample_features.corr()\n",
    "    im = axes[1, 2].imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    axes[1, 2].set_title('Feature Correlations (Sample)')\n",
    "    axes[1, 2].set_xticks(range(len(corr_matrix.columns)))\n",
    "    axes[1, 2].set_yticks(range(len(corr_matrix.columns)))\n",
    "    axes[1, 2].set_xticklabels(corr_matrix.columns, rotation=45, ha='right', fontsize=8)\n",
    "    axes[1, 2].set_yticklabels(corr_matrix.columns, fontsize=8)\n",
    "    plt.colorbar(im, ax=axes[1, 2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Basic features visualization completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f293ca",
   "metadata": {},
   "source": [
    "## 3. Advanced Feature Engineering\n",
    "\n",
    "Create advanced features including technical indicators, regime detection, and calendar effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6123850b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize advanced feature engineer\n",
    "advanced_engineer = AdvancedFeatureEngineer()\n",
    "\n",
    "print(\"Creating advanced features...\")\n",
    "print(\"This may take a while due to complex calculations...\")\n",
    "\n",
    "try:\n",
    "    # Create advanced features\n",
    "    advanced_features = advanced_engineer.create_features(target_data)\n",
    "    \n",
    "    print(f\"✓ Advanced features created\")\n",
    "    print(f\"Features shape: {advanced_features.shape}\")\n",
    "    print(f\"Feature columns: {list(advanced_features.columns)}\")\n",
    "    \n",
    "    # Display sample features\n",
    "    display(advanced_features.head())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Advanced feature creation failed: {e}\")\n",
    "    print(\"Continuing with basic features only...\")\n",
    "    advanced_features = pd.DataFrame(index=basic_features.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize advanced features (if available)\n",
    "if not advanced_features.empty and len(advanced_features.columns) > 0:\n",
    "    \n",
    "    # Select interesting features for visualization\n",
    "    feature_groups = {\n",
    "        'Technical': [col for col in advanced_features.columns if any(x in col.lower() for x in ['rsi', 'macd', 'bb'])],\n",
    "        'Volatility': [col for col in advanced_features.columns if 'vol' in col.lower()],\n",
    "        'Momentum': [col for col in advanced_features.columns if any(x in col.lower() for x in ['momentum', 'roc'])],\n",
    "        'Calendar': [col for col in advanced_features.columns if any(x in col.lower() for x in ['month', 'day', 'quarter'])]\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    for i, (group_name, features) in enumerate(feature_groups.items()):\n",
    "        if i >= 4:  # Only plot first 4 groups\n",
    "            break\n",
    "            \n",
    "        row, col = i // 2, i % 2\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        if features:\n",
    "            # Plot up to 3 features from each group\n",
    "            for feature in features[:3]:\n",
    "                if feature in advanced_features.columns:\n",
    "                    data = advanced_features[feature].dropna()\n",
    "                    if len(data) > 0:\n",
    "                        ax.plot(data.index, data.values, label=feature, alpha=0.8)\n",
    "            \n",
    "            ax.set_title(f'{group_name} Features')\n",
    "            ax.legend(fontsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, f'No {group_name}\\nFeatures Available', \n",
    "                   ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Advanced features visualization completed.\")\n",
    "    \n",
    "else:\n",
    "    print(\"No advanced features available for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a654e5",
   "metadata": {},
   "source": [
    "## 4. Volatility-Specific Features\n",
    "\n",
    "Create features specifically designed for volatility prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455e58b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize volatility estimators\n",
    "vol_estimator = VolatilityEstimators()\n",
    "\n",
    "print(\"Creating volatility-specific features...\")\n",
    "\n",
    "# Prepare OHLC data\n",
    "required_cols = ['Open', 'High', 'Low', 'Close']\n",
    "if all(col in target_data.columns for col in required_cols):\n",
    "    \n",
    "    ohlc_data = target_data[required_cols].copy()\n",
    "    print(f\"Using OHLC data: {ohlc_data.shape}\")\n",
    "    \n",
    "    # Calculate various volatility estimators\n",
    "    volatility_features = pd.DataFrame(index=ohlc_data.index)\n",
    "    \n",
    "    # Simple return volatility (multiple windows)\n",
    "    returns = ohlc_data['Close'].pct_change()\n",
    "    for window in [5, 10, 20, 30, 60]:\n",
    "        vol_simple = vol_estimator.simple_volatility(returns, window=window)\n",
    "        volatility_features[f'vol_simple_{window}d'] = vol_simple\n",
    "    \n",
    "    # High-frequency estimators\n",
    "    estimators = {\n",
    "        'parkinson': vol_estimator.parkinson_estimator,\n",
    "        'garman_klass': vol_estimator.garman_klass_estimator,\n",
    "        'rogers_satchell': vol_estimator.rogers_satchell_estimator,\n",
    "        'yang_zhang': vol_estimator.yang_zhang_estimator\n",
    "    }\n",
    "    \n",
    "    for name, estimator_func in estimators.items():\n",
    "        try:\n",
    "            if name == 'parkinson':\n",
    "                vol_est = estimator_func(ohlc_data['High'], ohlc_data['Low'])\n",
    "            else:\n",
    "                vol_est = estimator_func(\n",
    "                    ohlc_data['Open'], ohlc_data['High'], \n",
    "                    ohlc_data['Low'], ohlc_data['Close']\n",
    "                )\n",
    "            volatility_features[f'vol_{name}'] = vol_est\n",
    "            print(f\"✓ Created {name} volatility estimator\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Failed to create {name} estimator: {e}\")\n",
    "    \n",
    "    # Volatility of volatility\n",
    "    for col in volatility_features.columns:\n",
    "        if not volatility_features[col].empty:\n",
    "            vol_of_vol = volatility_features[col].rolling(20).std()\n",
    "            volatility_features[f'{col}_vol'] = vol_of_vol\n",
    "    \n",
    "    # Volatility ratios and spreads\n",
    "    vol_cols = [col for col in volatility_features.columns if col.startswith('vol_') and not col.endswith('_vol')]\n",
    "    \n",
    "    for i, col1 in enumerate(vol_cols):\n",
    "        for col2 in vol_cols[i+1:]:\n",
    "            # Ratio\n",
    "            ratio = volatility_features[col1] / volatility_features[col2]\n",
    "            volatility_features[f'{col1}_{col2}_ratio'] = ratio\n",
    "            \n",
    "            # Spread\n",
    "            spread = volatility_features[col1] - volatility_features[col2]\n",
    "            volatility_features[f'{col1}_{col2}_spread'] = spread\n",
    "    \n",
    "    print(f\"✓ Created {len(volatility_features.columns)} volatility features\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ OHLC data not complete, creating limited volatility features\")\n",
    "    volatility_features = pd.DataFrame(index=target_data.index)\n",
    "    \n",
    "    # Simple return volatility only\n",
    "    if 'Close' in target_data.columns:\n",
    "        returns = target_data['Close'].pct_change()\n",
    "        for window in [5, 10, 20, 30]:\n",
    "            vol_simple = vol_estimator.simple_volatility(returns, window=window)\n",
    "            volatility_features[f'vol_simple_{window}d'] = vol_simple\n",
    "\n",
    "print(f\"Volatility features shape: {volatility_features.shape}\")\n",
    "if not volatility_features.empty:\n",
    "    display(volatility_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c9e73f",
   "metadata": {},
   "source": [
    "## 5. Market-Specific Features\n",
    "\n",
    "Create features specific to Indian markets including VIX integration and calendar effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c171342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create market-specific features\n",
    "market_features = pd.DataFrame(index=target_data.index)\n",
    "\n",
    "print(\"Creating market-specific features...\")\n",
    "\n",
    "# VIX-based features\n",
    "if vix_data is not None and not vix_data.empty:\n",
    "    print(\"Adding VIX-based features...\")\n",
    "    \n",
    "    # Align VIX data with target data\n",
    "    vix_aligned = vix_data.reindex(target_data.index, method='ffill')\n",
    "    \n",
    "    if not vix_aligned.empty:\n",
    "        vix_col = vix_aligned.columns[0]  # Use first VIX column\n",
    "        \n",
    "        # VIX level\n",
    "        market_features['vix_level'] = vix_aligned[vix_col]\n",
    "        \n",
    "        # VIX changes\n",
    "        market_features['vix_change'] = vix_aligned[vix_col].pct_change()\n",
    "        market_features['vix_change_5d'] = vix_aligned[vix_col].pct_change(5)\n",
    "        \n",
    "        # VIX moving averages\n",
    "        for window in [5, 10, 20]:\n",
    "            market_features[f'vix_ma_{window}'] = vix_aligned[vix_col].rolling(window).mean()\n",
    "            market_features[f'vix_ma_{window}_ratio'] = vix_aligned[vix_col] / market_features[f'vix_ma_{window}']\n",
    "        \n",
    "        # VIX percentiles\n",
    "        for window in [30, 60, 252]:\n",
    "            market_features[f'vix_percentile_{window}d'] = vix_aligned[vix_col].rolling(window).rank(pct=True)\n",
    "        \n",
    "        print(f\"✓ Added {sum(1 for col in market_features.columns if 'vix' in col)} VIX features\")\n",
    "\n",
    "# Calendar effects (Indian market specific)\n",
    "print(\"Adding calendar effects...\")\n",
    "\n",
    "# Basic calendar features\n",
    "dates = pd.to_datetime(target_data.index)\n",
    "market_features['month'] = dates.month\n",
    "market_features['quarter'] = dates.quarter\n",
    "market_features['day_of_week'] = dates.dayofweek\n",
    "market_features['day_of_month'] = dates.day\n",
    "market_features['week_of_year'] = dates.isocalendar().week\n",
    "\n",
    "# Month-end and quarter-end effects\n",
    "market_features['is_month_end'] = (dates + pd.Timedelta(days=1)).month != dates.month\n",
    "market_features['is_quarter_end'] = (dates + pd.Timedelta(days=1)).quarter != dates.quarter\n",
    "market_features['days_to_month_end'] = (dates + pd.offsets.MonthEnd(0) - dates).dt.days\n",
    "market_features['days_to_quarter_end'] = (dates + pd.offsets.QuarterEnd(0) - dates).dt.days\n",
    "\n",
    "# Indian festival effects (simplified)\n",
    "# Note: This is a simplified version. In practice, you'd want a comprehensive Indian holiday calendar\n",
    "diwali_months = [10, 11]  # Diwali typically in Oct/Nov\n",
    "holi_months = [3, 4]      # Holi typically in Mar/Apr\n",
    "\n",
    "market_features['is_diwali_season'] = dates.month.isin(diwali_months)\n",
    "market_features['is_holi_season'] = dates.month.isin(holi_months)\n",
    "\n",
    "# Monsoon season (affects Indian markets)\n",
    "monsoon_months = [6, 7, 8, 9]  # June to September\n",
    "market_features['is_monsoon_season'] = dates.month.isin(monsoon_months)\n",
    "\n",
    "# Budget season effects\n",
    "budget_months = [2, 3]  # Budget typically in Feb/Mar\n",
    "market_features['is_budget_season'] = dates.month.isin(budget_months)\n",
    "\n",
    "print(f\"✓ Added {sum(1 for col in market_features.columns if col not in ['vix_level', 'vix_change', 'vix_change_5d'])} calendar features\")\n",
    "\n",
    "# Cross-asset features (if multiple symbols available)\n",
    "if len(stock_data) > 1:\n",
    "    print(\"Adding cross-asset features...\")\n",
    "    \n",
    "    # Calculate correlations with other major indices\n",
    "    other_symbols = [s for s in stock_data.keys() if s != target_symbol]\n",
    "    \n",
    "    target_returns = target_data['Close'].pct_change()\n",
    "    \n",
    "    for other_symbol in other_symbols[:3]:  # Limit to first 3 for performance\n",
    "        other_data = stock_data[other_symbol]\n",
    "        if 'Close' in other_data.columns:\n",
    "            other_returns = other_data['Close'].pct_change()\n",
    "            \n",
    "            # Rolling correlations\n",
    "            for window in [20, 60]:\n",
    "                corr = target_returns.rolling(window).corr(other_returns)\n",
    "                market_features[f'corr_{other_symbol.replace(\".NS\", \"\")}_{window}d'] = corr\n",
    "            \n",
    "            # Relative performance\n",
    "            rel_perf = target_returns - other_returns\n",
    "            market_features[f'rel_perf_{other_symbol.replace(\".NS\", \"\")}'] = rel_perf\n",
    "    \n",
    "    print(f\"✓ Added cross-asset features for {min(3, len(other_symbols))} symbols\")\n",
    "\n",
    "print(f\"Market features shape: {market_features.shape}\")\n",
    "if not market_features.empty:\n",
    "    display(market_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049d7e58",
   "metadata": {},
   "source": [
    "## 6. Combine All Features\n",
    "\n",
    "Combine all feature sets and perform final processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7964e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "print(\"Combining all features...\")\n",
    "\n",
    "feature_sets = {\n",
    "    'basic': basic_features,\n",
    "    'advanced': advanced_features,\n",
    "    'volatility': volatility_features,\n",
    "    'market': market_features\n",
    "}\n",
    "\n",
    "# Check which feature sets are available\n",
    "available_sets = {name: df for name, df in feature_sets.items() if not df.empty}\n",
    "\n",
    "print(f\"Available feature sets: {list(available_sets.keys())}\")\n",
    "for name, df in available_sets.items():\n",
    "    print(f\"  {name}: {df.shape[1]} features\")\n",
    "\n",
    "# Combine features\n",
    "if available_sets:\n",
    "    combined_features = pd.concat(list(available_sets.values()), axis=1)\n",
    "    \n",
    "    # Remove duplicate columns\n",
    "    combined_features = combined_features.loc[:, ~combined_features.columns.duplicated()]\n",
    "    \n",
    "    print(f\"\\n✓ Combined features shape: {combined_features.shape}\")\n",
    "    print(f\"Total features: {combined_features.shape[1]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No features available to combine\")\n",
    "    combined_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature quality assessment\n",
    "if not combined_features.empty:\n",
    "    print(\"=== FEATURE QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Missing value analysis\n",
    "    missing_analysis = pd.DataFrame({\n",
    "        'missing_count': combined_features.isnull().sum(),\n",
    "        'missing_pct': combined_features.isnull().sum() / len(combined_features) * 100\n",
    "    }).sort_values('missing_pct', ascending=False)\n",
    "    \n",
    "    print(f\"Features with >50% missing values: {(missing_analysis['missing_pct'] > 50).sum()}\")\n",
    "    print(f\"Features with >25% missing values: {(missing_analysis['missing_pct'] > 25).sum()}\")\n",
    "    print(f\"Features with >10% missing values: {(missing_analysis['missing_pct'] > 10).sum()}\")\n",
    "    \n",
    "    # Show worst missing value features\n",
    "    if missing_analysis['missing_pct'].max() > 0:\n",
    "        print(\"\\nTop 10 features with most missing values:\")\n",
    "        display(missing_analysis.head(10))\n",
    "    \n",
    "    # Constant features\n",
    "    numeric_features = combined_features.select_dtypes(include=[np.number])\n",
    "    constant_features = numeric_features.columns[numeric_features.std() == 0].tolist()\n",
    "    print(f\"\\nConstant features (std=0): {len(constant_features)}\")\n",
    "    if constant_features:\n",
    "        print(f\"Constant features: {constant_features[:10]}...\")  # Show first 10\n",
    "    \n",
    "    # Infinite values\n",
    "    inf_counts = np.isinf(numeric_features).sum()\n",
    "    features_with_inf = inf_counts[inf_counts > 0]\n",
    "    print(f\"\\nFeatures with infinite values: {len(features_with_inf)}\")\n",
    "    if len(features_with_inf) > 0:\n",
    "        print(f\"Features with inf values: {features_with_inf.head().to_dict()}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nFinal feature summary:\")\n",
    "    print(f\"  Total features: {combined_features.shape[1]}\")\n",
    "    print(f\"  Numeric features: {len(numeric_features.columns)}\")\n",
    "    print(f\"  Date range: {combined_features.index[0]} to {combined_features.index[-1]}\")\n",
    "    print(f\"  Total observations: {len(combined_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c20da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean features for modeling\n",
    "if not combined_features.empty:\n",
    "    print(\"Cleaning features for modeling...\")\n",
    "    \n",
    "    # Remove features with too many missing values (>50%)\n",
    "    missing_threshold = 0.5\n",
    "    missing_pct = combined_features.isnull().sum() / len(combined_features)\n",
    "    features_to_keep = missing_pct[missing_pct <= missing_threshold].index\n",
    "    \n",
    "    cleaned_features = combined_features[features_to_keep].copy()\n",
    "    print(f\"Removed {combined_features.shape[1] - len(features_to_keep)} features with >{missing_threshold*100}% missing values\")\n",
    "    \n",
    "    # Remove constant features\n",
    "    numeric_cols = cleaned_features.select_dtypes(include=[np.number]).columns\n",
    "    constant_cols = cleaned_features[numeric_cols].std() == 0\n",
    "    non_constant_cols = cleaned_features.columns[~cleaned_features.columns.isin(numeric_cols[constant_cols])]\n",
    "    \n",
    "    cleaned_features = cleaned_features[non_constant_cols]\n",
    "    print(f\"Removed {constant_cols.sum()} constant features\")\n",
    "    \n",
    "    # Replace infinite values with NaN\n",
    "    numeric_features = cleaned_features.select_dtypes(include=[np.number])\n",
    "    cleaned_features[numeric_features.columns] = numeric_features.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Forward fill missing values (simple strategy)\n",
    "    cleaned_features = cleaned_features.fillna(method='ffill')\n",
    "    \n",
    "    # Backward fill any remaining missing values\n",
    "    cleaned_features = cleaned_features.fillna(method='bfill')\n",
    "    \n",
    "    # Final missing value check\n",
    "    final_missing = cleaned_features.isnull().sum().sum()\n",
    "    print(f\"Final missing values: {final_missing}\")\n",
    "    \n",
    "    print(f\"\\n✓ Cleaned features shape: {cleaned_features.shape}\")\n",
    "    print(f\"Final feature count: {cleaned_features.shape[1]}\")\n",
    "    \n",
    "    # Display final feature summary\n",
    "    if len(cleaned_features.columns) > 0:\n",
    "        print(\"\\nFinal features (first 10):\")\n",
    "        display(cleaned_features.iloc[:, :10].head())\n",
    "        \n",
    "        print(\"\\nFeature statistics (first 10):\")\n",
    "        display(cleaned_features.iloc[:, :10].describe())\n",
    "        \n",
    "else:\n",
    "    print(\"⚠ No features available for cleaning\")\n",
    "    cleaned_features = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4022d1",
   "metadata": {},
   "source": [
    "## 7. Save Engineered Features\n",
    "\n",
    "Save all engineered features for use in model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8c7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save engineered features\n",
    "if not cleaned_features.empty:\n",
    "    \n",
    "    # Save cleaned features\n",
    "    features_file = os.path.join(processed_data_dir, f'{target_symbol.replace(\".NS\", \"\")}_engineered_features.csv')\n",
    "    cleaned_features.to_csv(features_file)\n",
    "    print(f\"✓ Saved engineered features to {os.path.basename(features_file)}\")\n",
    "    \n",
    "    # Save feature metadata\n",
    "    feature_metadata = {\n",
    "        'creation_timestamp': datetime.now().isoformat(),\n",
    "        'target_symbol': target_symbol,\n",
    "        'total_features': cleaned_features.shape[1],\n",
    "        'total_observations': cleaned_features.shape[0],\n",
    "        'date_range': {\n",
    "            'start': cleaned_features.index[0].isoformat(),\n",
    "            'end': cleaned_features.index[-1].isoformat()\n",
    "        },\n",
    "        'feature_sets': {\n",
    "            name: {\n",
    "                'feature_count': df.shape[1],\n",
    "                'features': list(df.columns)\n",
    "            }\n",
    "            for name, df in available_sets.items()\n",
    "        },\n",
    "        'missing_value_threshold': missing_threshold,\n",
    "        'final_missing_values': final_missing,\n",
    "        'all_features': list(cleaned_features.columns)\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    import json\n",
    "    metadata_file = os.path.join(processed_data_dir, f'{target_symbol.replace(\".NS\", \"\")}_feature_metadata.json')\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(feature_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"✓ Saved feature metadata to {os.path.basename(metadata_file)}\")\n",
    "    \n",
    "    # Create feature importance analysis (simple correlation with volatility)\n",
    "    if 'vol_simple_20d' in cleaned_features.columns:\n",
    "        target_vol = cleaned_features['vol_simple_20d']\n",
    "        numeric_features = cleaned_features.select_dtypes(include=[np.number])\n",
    "        \n",
    "        feature_correlations = numeric_features.corrwith(target_vol).abs().sort_values(ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 features correlated with 20-day volatility:\")\n",
    "        display(feature_correlations.head(10))\n",
    "        \n",
    "        # Save feature correlations\n",
    "        corr_file = os.path.join(processed_data_dir, f'{target_symbol.replace(\".NS\", \"\")}_feature_correlations.csv')\n",
    "        feature_correlations.to_csv(corr_file)\n",
    "        print(f\"✓ Saved feature correlations to {os.path.basename(corr_file)}\")\n",
    "    \n",
    "    print(\"\\n=== FEATURE ENGINEERING COMPLETED ===\")\n",
    "    print(f\"Successfully created {cleaned_features.shape[1]} features for {target_symbol}\")\n",
    "    print(f\"Data covers {cleaned_features.shape[0]} observations\")\n",
    "    print(f\"All files saved to: {processed_data_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠ No features to save\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
